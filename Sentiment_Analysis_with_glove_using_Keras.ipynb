{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c4da2ac09e22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from pathlib import Path\n",
    "from numpy.random import random, permutation, randn, normal, uniform, choice\n",
    "from keras import applications\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image, sequence, text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.manifold\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the pretrained word vectors trained on 6 billion tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the glove vectors from here https://github.com/stanfordnlp/GloVe and place the data in glove_dir. \n",
    "#Choose glove.6B.zip\n",
    "\n",
    "glove_dir = '/home/ubuntu/glove/'\n",
    "glove_file = 'glove.6B.50d'\n",
    "glove_path = glove_dir + glove_file + '.txt'\n",
    "\n",
    "with open(glove_path) as myfile:\n",
    "    head = [next(myfile) for x in range(3)]\n",
    "print(head[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab from the GloVe embedding file\n",
    "\n",
    "words_pkl = glove_dir + glove_file + '.wpkl'\n",
    "vecs_pkl = glove_dir + glove_file + '.vpkl'\n",
    "wordidx_pkl = glove_dir + glove_file + '.ipkl'\n",
    "\n",
    "def create_glove_vocab(path, w_pkl, v_pkl, i_pkl):\n",
    "    words = []\n",
    "    wordidx = {}\n",
    "    emb_vecs = {}\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        i = 0\n",
    "        for line in f.readlines():\n",
    "            data = line.strip().split(' ')\n",
    "            words.append(data[0])\n",
    "            wordidx[data[0]] = i\n",
    "            emb_vecs[i] = np.asarray([float(val) for val in data[1:]])         \n",
    "            i += 1\n",
    "    \n",
    "    vecs = np.stack((v for k, v in emb_vecs.items()),axis=1)\n",
    "    vecs = vecs.transpose()\n",
    "    \n",
    "    with open(w_pkl, 'wb') as f1:\n",
    "        pickle.dump(words, f1, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"created word file\")\n",
    "        \n",
    "    with open(v_pkl, 'wb') as f2:\n",
    "        pickle.dump(vecs, f2, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"created word vector file\")\n",
    "        \n",
    "    with open(i_pkl, 'wb') as f3:\n",
    "        pickle.dump(wordidx, f3, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"created word index file\")\n",
    "    \n",
    "    return vecs, words, wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found files\n",
      "loaded vocab file\n",
      "loaded embed file\n",
      "loaded index file\n"
     ]
    }
   ],
   "source": [
    "if Path(words_pkl).is_file() and Path(vecs_pkl).is_file() and Path(wordidx_pkl).is_file():\n",
    "    print(\"found files\")\n",
    "    with open(words_pkl, 'rb') as f1:\n",
    "        words = pickle.load(f1)\n",
    "        print(\"loaded vocab file\")\n",
    "        \n",
    "    with open(vecs_pkl, 'rb') as f2:\n",
    "        vecs = pickle.load(f2)\n",
    "        print(\"loaded embed file\")\n",
    "        \n",
    "    with open(wordidx_pkl, 'rb') as f3:\n",
    "        wordidx = pickle.load(f3)\n",
    "        print(\"loaded index file\")\n",
    "        \n",
    "else:\n",
    "    vecs, words, wordidx = create_glove_vocab(glove_path, words_pkl, vecs_pkl, wordidx_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_range = 20000\n",
    "short_vecs = vecs[:vocab_range]\n",
    "short_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_words = words[:vocab_range]\n",
    "len(short_words)\n",
    "short_words[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of Amazon reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train and test files are opened and then the readlines returns a list of all lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file= '/home/ubuntu/Best/train.ft.txt'\n",
    "test_file= '/home/ubuntu/Best/test.ft.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_o= open(train_file, 'r')\n",
    "test_o= open(test_file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_o.readlines()\n",
    "test = test_o.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The labels in the datasets are assigned sentiments (0/1) and the reviews are cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maximum words are restricted to only 10000 since anything greater took a long time for the model to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tokenizer function vectorizes the corpus into a sequence of integers according to the number of words limit, based on word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token = tokenizer.texts_to_sequences(train_sentences)\n",
    "type(train_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74,\n",
       " 11,\n",
       " 1,\n",
       " 610,\n",
       " 6395,\n",
       " 8,\n",
       " 176,\n",
       " 485,\n",
       " 13,\n",
       " 364,\n",
       " 6,\n",
       " 6071,\n",
       " 1,\n",
       " 10,\n",
       " 60,\n",
       " 438,\n",
       " 27,\n",
       " 69,\n",
       " 3,\n",
       " 39,\n",
       " 1835,\n",
       " 6,\n",
       " 74,\n",
       " 5,\n",
       " 135,\n",
       " 72,\n",
       " 678,\n",
       " 145,\n",
       " 119,\n",
       " 3,\n",
       " 20,\n",
       " 524,\n",
       " 1,\n",
       " 145,\n",
       " 1870,\n",
       " 16,\n",
       " 40,\n",
       " 7,\n",
       " 28,\n",
       " 7,\n",
       " 1,\n",
       " 597,\n",
       " 3,\n",
       " 20,\n",
       " 132,\n",
       " 524,\n",
       " 6,\n",
       " 44,\n",
       " 1,\n",
       " 88,\n",
       " 119,\n",
       " 6,\n",
       " 6682,\n",
       " 240,\n",
       " 37,\n",
       " 5109,\n",
       " 2,\n",
       " 422,\n",
       " 4,\n",
       " 851,\n",
       " 17,\n",
       " 8995,\n",
       " 3028,\n",
       " 2,\n",
       " 4338,\n",
       " 6,\n",
       " 39,\n",
       " 5209,\n",
       " 207,\n",
       " 72,\n",
       " 2728,\n",
       " 5,\n",
       " 336]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we limit the vocab size and all less common words are clubbed together for making the model to run faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in train_token]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in test_token]\n",
    "type(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(len,trn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(map(len,trn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.43316833333333"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "mean(map(len,trn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we truncate the reviews to max 500 words. We need to pad the reviews which are shorter so that we have uniform length inputs to feed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "X_train = sequence.pad_sequences(trn, maxlen=seq_len)\n",
    "X_test = sequence.pad_sequences(test, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,   74,   11,\n",
       "          1,  610, 6395,    8,  176,  485,   13,  364,    6, 6071,    1,\n",
       "         10,   60,  438,   27,   69,    3,   39, 1835,    6,   74,    5,\n",
       "        135,   72,  678,  145,  119,    3,   20,  524,    1,  145, 1870,\n",
       "         16,   40,    7,   28,    7,    1,  597,    3,   20,  132,  524,\n",
       "          6,   44,    1,   88,  119,    6, 6682,  240,   37, 5109,    2,\n",
       "        422,    4,  851,   17, 7999, 3028,    2, 4338,    6,   39, 5209,\n",
       "        207,   72, 2728,    5,  336], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets build our models with different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st & 2nd models - we choose default keras embedding which is uniform distribution. The difference between the two is the size of the embeddings. The batch size is kept high since there are a lot of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           256000    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 100)               53600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 309,701\n",
      "Trainable params: 309,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "model1 = Sequential([\n",
    "    Embedding(vocab_size, embed_dim, input_length=seq_len),\n",
    "    CuDNNLSTM(100),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3600000 samples, validate on 400000 samples\n",
      "Epoch 1/1\n",
      "3600000/3600000 [==============================] - 929s 258us/step - loss: 0.2165 - acc: 0.9149 - val_loss: 0.1909 - val_acc: 0.9254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc10c221320>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model1.fit(X_train, train_labels, validation_data=(X_test, test_labels), epochs=1, batch_size=4096) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 50)           400000    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 100)               60800     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 460,901\n",
      "Trainable params: 460,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 50\n",
    "model2 = Sequential([\n",
    "    Embedding(vocab_size, embed_dim, input_length=seq_len),\n",
    "    CuDNNLSTM(100),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3600000 samples, validate on 400000 samples\n",
      "Epoch 1/1\n",
      "3600000/3600000 [==============================] - 1029s 286us/step - loss: 0.2489 - acc: 0.8978 - val_loss: 0.1854 - val_acc: 0.9281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc10c22beb8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model2.fit(X_train, train_labels, validation_data=(X_test, test_labels), epochs=1, batch_size=4096) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the 3rd model we choose the inital word embeddings from the pre trained word vectors of GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "    new_word_count = 0\n",
    "    nw_idx = []\n",
    "    new_words = []\n",
    "    for i in range(1, vocab_size):\n",
    "        word = train_token[i]\n",
    "        if word in words:\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(n_fact,))\n",
    "            new_word_count += 1\n",
    "            nw_idx.append(i)\n",
    "            new_words.append(word)\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb, new_word_count, nw_idx, new_words      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb, new_word_count, nw_idx, new_words = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7999"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = emb.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 50)           400000    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 100)               60800     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 460,901\n",
      "Trainable params: 460,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential([\n",
    "    Embedding(vocab_size, embed_dim, weights=[emb], input_length=seq_len),\n",
    "    CuDNNLSTM(100),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3600000 samples, validate on 400000 samples\n",
      "Epoch 1/1\n",
      "3600000/3600000 [==============================] - 1032s 287us/step - loss: 0.2743 - acc: 0.8845 - val_loss: 0.1912 - val_acc: 0.9261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0fdaf1d30>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model3.fit(X_train, train_labels, validation_data=(X_test, test_labels), epochs=1, batch_size=4096) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just run both models for one more epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### it seems that model2 is the best with 93% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.81%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "scores = model2.evaluate(X_test, test_labels, verbose=0) \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two sample random reviews are used to predict sentiments from the best model. Both of them are accurately predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The coffee tasted great and was at such a good price! I highly recommend this to everyone!']. Sentiment: [0.98861796]\n",
      "[\"I have sent numerous emails to the company. I can't actually find a phone number on their website - and I still have not gotten any kind of response.\"]. Sentiment: [0.02131079]\n"
     ]
    }
   ],
   "source": [
    "#predict sentiment from reviews\n",
    "good = [\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\"]\n",
    "bad = [\"I have sent numerous emails to the company. I can't actually find a phone number on their website - and I still have not gotten any kind of response.\"]\n",
    "#putting both the reviews in a loop\n",
    "for review in [good,bad]:\n",
    "    #vectorizing the review by the pre-fitted tokenizer instance\n",
    "    review_1 = tokenizer.texts_to_sequences(review)\n",
    "    #padding the review to have exactly the same shape as `embedding_2` input\n",
    "    review_2 = sequence.pad_sequences(review_1, maxlen=500, dtype='int32', value=0)\n",
    "    print(\"%s. Sentiment: %s\" % (review,model2.predict(review_2,batch_size=1,verbose = 2)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
